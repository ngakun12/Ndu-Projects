---
title: "R Notebook"
output: html_notebook
---

```{r}
# Load necessary library
library(ggplot2)

# Generating pm1 values from 0 to 1
pm1 <- seq(0, 1, by = 0.01)

# Calculating Gini index, Classification error, and Entropy
gini_index <- 2 * pm1 * (1 - pm1)
classification_error <- 1 - pmax(pm1, 1 - pm1)
entropy <- -pm1 * log2(pm1) - (1 - pm1) * log2(1 - pm1)

# Replacing NaN with 0 for log(0) case in entropy
entropy[is.na(entropy)] <- 0

# Preparing data for ggplot
df <- data.frame(pm1, gini_index, classification_error, entropy)

# Melting the data for easy plotting with ggplot
df_melted <- reshape2::melt(df, id.vars = 'pm1')

# Plotting
ggplot(df_melted, aes(x = pm1, y = value, color = variable)) +
  geom_line() +
  labs(x = expression(hat(pm)[1]), y = "Value", title = "Gini Index, Classification Error, and Entropy") +
  scale_color_manual(values = c("gini_index" = "blue", "classification_error" = "red", "entropy" = "green"), labels = c("Gini Index", "Classification Error", "Entropy")) +
  theme_minimal()

```
The process of fitting a regression tree is a methodical approach that starts with the entire dataset represented at the root node. The core of the algorithm involves iteratively selecting the best feature and split point to partition the data, aiming to maximize variance reduction at each step. This selection process considers each possible feature and its potential split points, evaluating them based on their ability to segregate the dataset such that subsets contain observations with similar response values, thus minimizing the variance within these subsets.

Once the optimal split is determined, the dataset is divided into two subsets, and this procedure of recursive binary splitting continues. The algorithm further splits each resulting subset, adhering to the same criteria for selecting the best split, and this recursive process persists until certain predefined stopping conditions are met. These conditions include reaching a minimum number of observations in a node, a situation where further splits do not significantly reduce the variance, or achieving a maximum depth of the tree.

To ensure the model does not overfit the data, a pruning phase may follow the tree's growth. Pruning involves trimming down the fully grown tree by removing splits that contribute minimally to the model's predictive power. This is regulated by a complexity parameter, which is optimized through cross-validation to find a balance between the tree's fit to the training data and its simplicity.

The final model, represented by the pruned tree, uses piecewise constant approximations for predictions. An observation is directed down the tree to a leaf node based on its feature values and the splits defined within the tree. The mean of the target variable for the training observations in that leaf node then becomes the prediction for the observation. Through this process, the regression tree algorithm captures complex, non-linear relationships in the data with a series of simple, interpretable rules, yielding a model that is not only adaptable but also straightforward for users to understand. This combination of adaptability and interpretability makes regression trees a valuable tool in the repertoire of machine learning methods.
```{r}
install.packages("rpart")
library(rpart)

```



```{r}

library(ISLR)

```
```{r}
data("OJ")


str(OJ)
```
```{r}
set.seed(123) # Ensure reproducibility

# Create a random sample of 800 observations for the training set
train_indices <- sample(1:nrow(OJ), 800)

# Split the data into training and test sets
train_data <- OJ[train_indices, ]
test_data <- OJ[-train_indices, ]

```

```{r}
# Fit the decision tree model
tree_model <- rpart(Purchase ~ ., data = train_data, method="class")

# Check the summary of the model
summary(tree_model)

```
```{r}
# Assuming you have already loaded the OJ dataset from the ISLR package
# Install and load the ISLR package if you haven't already
# install.packages("ISLR")
library(ISLR)

# Load the OJ dataset
data(OJ)

# Setting a seed for reproducibility
set.seed(123)

# Create indices for randomly sampling 800 observations for the training set
train_indices <- sample(nrow(OJ), 800)

# Create the training and test sets
train_data <- OJ[train_indices, ]
test_data <- OJ[-train_indices, ]

# Install and load the rpart package for fitting a tree model
# install.packages("rpart")
library(rpart)

# Fit a tree model to the training data
model <- rpart(Purchase ~ ., data = train_data, method = "class")

# Predict class labels for the training data
predictions <- predict(model, train_data, type = "class")

# Calculate the training error rate
train_error_rate <- mean(predictions != train_data$Purchase)


```

```{r}
# Print the training error rate
print(train_error_rate)
```

.15 is the error rate

```{r}
# Plot the tree
plot(model, main="Decision Tree")
text(model, use.n=TRUE)

```
There are five terminal nodes
Terminal nodes usually contain the predicted class for that node and often the distribution or count of the class labels from the training data that fell into that node. For example, in the leftmost node, '381/42' suggests that 381 observations from the training set were predicted as 'CH' and 42 as 'MM'. This count can help assess the purity of the node - the higher the count of the majority class relative to the other, the purer the node.



```{r}
install.packages("rattle")



```
```{r}
# install.packages("rpart.plot") # Uncomment if you haven't installed the package
library(rpart.plot)

# Print a detailed text summary of the tree structure
rpart.plot(model, type = 4, extra = 101)

```
```{r}
# Print a summary of the tree
summary(model)

```
I picked node number 4 

  Predicted Class: CH. The model predicts 'CH' for observations falling into this node.
  Expected Loss: 0.09929078. This is the error rate for misclassification at this node â€“ about 9.93%. It's relatively low, indicating a good level of confidence in the predictions at this node.
    P(node): 0.52875. This value represents the proportion of observations from the training data that fall into this node, which is roughly 52.88%.
    Class Counts: 381 for CH and 42 for MM. There are 381 observations labeled 'CH' and 42 observations labeled 'MM' in this node.
    Probabilities: 0.901 for CH and 0.099 for MM. Given an observation falls into this node, there is a 90.1% chance it's classified as 'CH' and a 9.9% chance it's classified as 'MM'.

This node does not split further and is a terminal node, meaning it does not branch out into more decisions. Observations that fall into this node are classified based on the highest probability which, in this case, is for class 'CH'. The classification is made with a high confidence level (90.1%), and the error rate for this group of observations is quite low, suggesting that the features leading to this node are strong predictors for the 'CH' class.




```{r}
# Assuming 'model' is your trained decision tree and 'test_data' is your test dataset

# Predict class labels for the test data
test_predictions <- predict(model, test_data, type = "class")

# Create a confusion matrix
confusion_matrix <- table(Predicted = test_predictions, Actual = test_data$Purchase)

# Print the confusion matrix
print(confusion_matrix)

# Calculate and print the test error rate
test_error_rate <- sum(test_predictions != test_data$Purchase) / length(test_predictions)
print(paste("Test error rate:", test_error_rate))

```
Test error rate is 0.181481481481481


```{r}
# Load necessary libraries
library(caret)
library(rpart)

# Set up cross-validation control
train_control <- trainControl(method="cv", number=10)

# Train the model with cross-validation to select the optimal complexity parameter (cp)
model_cv <- train(Purchase ~ ., 
                  data=train_data, 
                  method="rpart", 
                  trControl=train_control,
                  tuneLength=10)

# Print the results
print(model_cv)

```
```{r}
# Assuming 'model_cv' is the model trained with cross-validation as shown in the previous example
# Extract the results
results <- model_cv$results

# Plot cross-validated classification error rate vs. tree size (1/Complexity Parameter)
plot(1/results$cp, results$Accuracy, type='b', col='blue', 
     xlab='Tree Size (1/Complexity Parameter)', ylab='Cross-Validated Accuracy',
     main='Tree Size vs. Cross-Validated Accuracy')

```
```{r}
# Assuming model_cv is the model trained with cross-validation
# Find the row with the highest accuracy
best_model_row <- which.max(model_cv$results$Accuracy)

# Extract the best cp value
best_cp <- model_cv$results$cp[best_model_row]

# Calculate the corresponding tree size (1/cp)
best_tree_size <- 1 / best_cp

# Print the best cp value and the corresponding tree size
print(paste("Best cp:", best_cp))
print(paste("Corresponding tree size (1/cp):", best_tree_size))

```
[1] "Best cp: 0"
[1] "Corresponding tree size (1/cp): Inf"


```{r}
# Assuming 'model' is your original tree model and 'best_cp' is the optimal cp value you've identified
# Prune the tree using the best_cp value
pruned_model <- prune(model, cp = best_cp)

# Plot the pruned tree
plot(pruned_model, main="Pruned Decision Tree")
text(pruned_model, use.n=TRUE)

# Optionally, if you want to see the summary of the pruned tree
print(summary(pruned_model))

```



```{r}
# Predict class labels for the training data using the pruned tree
pruned_predictions <- predict(pruned_model, train_data, type = "class")

# Calculate the training error rate for the pruned tree
pruned_train_error_rate <- mean(pruned_predictions != train_data$Purchase)

# Print the training error rate for the pruned tree
print(pruned_train_error_rate)

```
they have the same error rate



```{r}
# For the unpruned tree
unpruned_predictions <- predict(model, newdata = test_data, type = "class")
unpruned_test_error_rate <- mean(unpruned_predictions != test_data$Purchase)
print(paste("Unpruned Test Error Rate:", unpruned_test_error_rate))

# For the pruned tree
pruned_predictions <- predict(pruned_model, newdata = test_data, type = "class")
pruned_test_error_rate <- mean(pruned_predictions != test_data$Purchase)
print(paste("Pruned Test Error Rate:", pruned_test_error_rate))

```


same test error rate
